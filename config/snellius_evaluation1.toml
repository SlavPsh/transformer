# example toml
[experiment]
name = "Transformer with flex attention"
description = "Evaluation"

[data]
#feature_cols = ["x", "y", "z", "cos_theta", "sin_phi", "cos_phi", "q", "log_p", "vz", "log_pt", "1/pt", "pt", "eta", "particle_id", "weight", "event_id", "surface_id", "cluster_id"]
feature_cols =   ["x", "y", "z", 'xi','yi','zi','dirx','diry','dirz', "vz", "pt", "eta", "particle_id",
        "weight", "event_id", "surface_id", "cluster_id"]
data_dir = "/projects/0/nisei0750/slava/data/ml_input_data/batched_clustered_data_DBSCAN_0107/tensors_barrel"
normalize = true
dataloader_num_workers = 0


[data.normalization_features]

[data.normalization_features.x]
mean = 0
std = 91

[data.normalization_features.y]
mean = 0
std = 91

[data.normalization_features.z]
mean = 0
std = 200

[data.normalization_features.xi]
mean = 0
std = 91

[data.normalization_features.yi]
mean = 0
std = 91

[data.normalization_features.zi]
mean = 0
std = 200



[model]
checkpoint_path = "/projects/0/nisei0750/slava/data/trained_models/20250722_182348_mottled-dainty-echidna_train/model_best.pth"

# type = "vanilla"
type = "flex_attention"
num_encoder_layers = 12
d_model = 192
dim_feedforward = 384
n_head = 4
input_features = ['x', 'y', 'z']
output_features = ['xi','yi', 'zi', 'dirx', 'diry', 'dirz']
output_size = 32
dropout = 0.1
mask_on_clusters = false

[training]
batch_size = 1

[output]
base_path = "/projects/0/nisei0750/slava/data/evaluation"

[logging]
level = "INFO"

[wandb]

[sweep]
entity = "spshenov-university-of-amsterdam"
project = "transformer_with_flex_attention"
method = "grid"  # or 'random', 'bayes'

[sweep.metric]
name = "test/event score"
goal = "maximize"

[sweep.parameters.epsilon]
#values = [1.0, 0.0001, 10.0, 0.001]
values = [0.05]

#[sweep.parameters.min_cl_size]
#values = [1]

[sweep.parameters.min_samples]
values = [5]

[bin_ranges.pt]
min = 1
max = 5
step = 0.5

[bin_ranges.eta]
min = 0
max = 2.2
step = 0.2
