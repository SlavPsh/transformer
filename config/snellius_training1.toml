# Configuration toml
[experiment]
name = "Transformer with attention distance"
description = "Training model on vertex data"

[data]
data_dir = "/projects/0/nisei0750/slava/data/ml_input_data"
# trackml_tiny_10to50tracks_100events.csv
# trackml_short_10to50_tracks_5k_events.csv
# trackml_10to50tracks_40kevents.csv
# trackml_200to500tracks_40kevents.csv
data_file = "trackml_2to10vertices.csv"
dataloader_num_workers = 4

[model]
num_encoder_layers = 6
d_model = 128
n_head = 4
input_size = 3
output_size = 4
dim_feedforward = 256
dropout = 0.1
use_flash_attention = false
use_att_mask = false

[training]
batch_size = 1
total_epochs = 500
shuffle = false # whether training data should be shuffled
start_from_scratch = true
default_lr = 1e-3

[training.early_stopping]
patience = 100

[output]
base_path = "/projects/0/nisei0750/slava/data/trained_models"

[logging]
level = "INFO"
epoch_log_interval = 10
model_save_interval = 30

[wandb]
watch_interval = 1

[sweep]
entity = "spshenov-university-of-amsterdam"
project = "transformer_for_particle_tracking"
method = "grid"

[sweep.metric]
name = "train/validation loss"
goal = "minimize"

[sweep.parameters.use_att_mask]
values = [false]