# Configuration toml
[experiment]
name = "Transformer with attention distance"
description = "Training a model with attention distance mask"

[data]
data_dir = "/data/atlas/users/spshenov/ml_input_data"
# trackml_tiny_10to50tracks_100events.csv
# trackml_short_10to50_tracks_5k_events.csv
# trackml_10to50tracks_40kevents.csv
data_file = "trackml_10to50tracks_40kevents.csv"
dataloader_num_workers = 4

[model]
num_encoder_layers = 6
d_model = 32
n_head = 4
input_size = 3
output_size = 5
dim_feedforward = 128
dropout = 0.1
use_flash_attention = false
use_att_mask = false

[training]
batch_size = 64
total_epochs = 660
shuffle = false # whether training data should be shuffled
start_from_scratch = true
default_lr = 4e-4

[training.early_stopping]
patience = 60

[output]
base_path = "/data/atlas/users/spshenov/trained_models"

[logging]
level = "INFO"
epoch_log_interval = 10
model_save_interval = 30

[wandb]
entity = "spshenov-university-of-amsterdam"
project_name = "transformer_for_particle_tracking"
watch_interval = 50