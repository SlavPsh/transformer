# Configuration toml
[experiment]
name = "Transformer with attention distance"
description = "Training a model with attention distance mask"

[data]
data_dir = "/data/atlas/users/spshenov/ml_input_data"
# trackml_tiny_10to50tracks_100events.csv
# trackml_short_10to50_tracks_5k_events.csv
# trackml_10to50tracks_40kevents.csv
data_file = "trackml_tiny_10to50tracks_100events.csv"
val_file = "sorted_val.pt"
test_file = "sorted_test.pt"
test_helperfile = "sorted_test_helper.pt"
test_truthfile = "test_truths.csv"
dataloader_num_workers = 4

[model]
num_encoder_layers = 6
d_model = 32
n_head = 4
input_size = 3
output_size = 5
dim_feedforward = 128
dropout = 0.1
use_flash_attention = false
use_att_mask = true

[training]
batch_size = 64
total_epochs = 3
shuffle = false # whether training data should be shuffled
start_from_scratch = true
default_lr = 1e-3

[training.early_stopping]
patience = 30

[output]
base_path = "/data/atlas/users/spshenov/studio/trained_models"

[logging]
level = "INFO"
epoch_log_interval = 1
model_save_interval = 1

[wandb]
entity = "spshenov-university-of-amsterdam"
project_name = "transformer_for_particle_tracking"
watch_interval = 1